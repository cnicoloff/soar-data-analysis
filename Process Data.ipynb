{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# autoreload reloads modules automatically before entering the execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "\n",
    "# Show plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import halfnorm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Import KML\n",
    "from pykml.factory import KML_ElementMaker as KML\n",
    "from pykml.factory import GX_ElementMaker as GX\n",
    "from pykml.parser import Schema\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Formatting Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "plt.rcParams['figure.figsize'] = (15.0, 10.0)\n",
    "plt.rcParams['axes.titlesize'] = 28\n",
    "plt.rcParams['axes.labelsize'] = 24\n",
    "plt.rcParams['xtick.labelsize'] = 20\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "plt.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ten minutes with a Cs-137 source\n",
    "#data_dir = r\".\\2018-05-03\"\n",
    "#data_file = \"%s\\counts_cesium_ten_minutes.txt\" % (data_dir)\n",
    "\n",
    "# Data from the tethered launch\n",
    "#data_dir = r\".\\2018-05-08\"\n",
    "#data_file = \"%s\\counts_tethered_launch.txt\" % (data_dir)\n",
    "\n",
    "# Data from the launch!\n",
    "data_dir = r\".\\2018-05-13\"\n",
    "counts_file = \"%s/counts_launch.txt\" % (data_dir)\n",
    "star_file = \"%s/counts_launch.csv\" % (data_dir)\n",
    "tel_file = \"%s/telemetrydata.txt\" % (data_dir)\n",
    "gps_file = \"%s/pits_gps.txt\" % (data_dir)\n",
    "pits_file = \"%s/pits_images.txt\" % (data_dir)\n",
    "msi_file = \"%s/msi_images.txt\" % (data_dir)\n",
    "\n",
    "msi_seeds = np.array([int(391900854), int(18925409), int(315370512)])\n",
    "\n",
    "joined_file = \"%s/soar_data_joined.csv\" % (data_dir)\n",
    "\n",
    "# Reading the telemetry file\n",
    "pat = \"^\\$\\$KC1FNU,\\d*,(\\d+)\\:(\\d+)\\:(\\d+),([0-9.]*),-([0-9.]*),(\\d*),[0-9,.-]*\\*([0-9A-F]*)$\"\n",
    "\n",
    "# Reading the GPS file\n",
    "pat1 = \"^\\$GNGGA,(\\d{2})(\\d{2})(\\d{2}).\\d{2},(\\d{2})([0-9.]{1,}),N,(\\d{3})([0-9.]{1,}),W,(\\d{1,}),(\\d{1,}),([0-9.]{1,}),([0-9.]{1,}),M,[0-9.-]{1,},M,.*?$\"\n",
    "\n",
    "# Reading the list of PITS images\n",
    "pat2 = \"^.*?\\s{1,}((\\d{2})_(\\d{2})_(\\d{2})-rot.jpg)\\s*$\"\n",
    "\n",
    "# Reading the list of MSI images\n",
    "pat3 = \"^.*?\\s{1,}(\\d{4}-\\d{2}-\\d{2}_(\\d{2})(\\d{2})(\\d{2})-(\\d{1,})-cam(\\d{1}).jpg)\\s*$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See if a value is a number or not\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Convert a number to float or int\n",
    "def cvt_number(s):\n",
    "    if ('.' in s):\n",
    "        num = float(s)\n",
    "    else:\n",
    "        num = int(s)\n",
    "    return num\n",
    "\n",
    "# Round a number to a certain number of decimal places\n",
    "def round_precision(num, precision):\n",
    "    p10 = 10**precision\n",
    "    num = num * p10\n",
    "    num = np.ceil(num)\n",
    "    num = num / p10\n",
    "    return num\n",
    "\n",
    "# Calculate a running mean/moving average\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "# Calculate the average deadtime per interval\n",
    "def avg_deadtime(dead_time, deadt_counts):\n",
    "    # Divide in such a way that division by zero doesn't halt the program\n",
    "    # Instead, it returns a zero\n",
    "    avg_deadt = np.divide(dead_time, deadt_counts, out=np.zeros_like(dead_time), where=deadt_counts!=0)\n",
    "    return avg_deadt\n",
    "\n",
    "# When HV isn't on, the counts are -1, so trim those entries\n",
    "def enforce_min_counts(seconds, counts, dead_time, deadt_counts, T_raw, T, P_raw, P1, P2, alt, min_counts):\n",
    "    seconds = np.array(seconds[np.where(deadt_counts >= min_counts)])\n",
    "    counts = np.array(counts[np.where(deadt_counts >= min_counts)])\n",
    "    T_raw = np.array(T_raw[np.where(deadt_counts >= min_counts)])\n",
    "    T = np.array(T[np.where(deadt_counts >= min_counts)])\n",
    "    P_raw = np.array(P_raw[np.where(deadt_counts >= min_counts)])\n",
    "    P1 = np.array(P1[np.where(deadt_counts >= min_counts)], dtype='float64')\n",
    "    P2 = np.array(P2[np.where(deadt_counts >= min_counts)], dtype='float64')\n",
    "    alt = np.array(alt[np.where(deadt_counts >= min_counts)])\n",
    "    dead_time = np.array(dead_time[np.where(deadt_counts >= min_counts)])\n",
    "    deadt_counts = np.array(deadt_counts[np.where(deadt_counts >= min_counts)])\n",
    "    seconds_dt = np.gradient(seconds)\n",
    "    \n",
    "    return seconds, seconds_dt, counts, dead_time, deadt_counts, T_raw, T, P_raw, P1, P2, alt\n",
    "\n",
    "# Find all indices that have a minimum number of counts\n",
    "def loc_min_counts(counts, min_counts):\n",
    "    return np.where(counts >= min_counts)\n",
    "\n",
    "# Corrects counts for dead time, avg_deadt can be a scalar or a per-interval array\n",
    "def deadt_correction(seconds_dt, deadt_counts, avg_deadt):\n",
    "    counts_c = deadt_counts / (1 - (deadt_counts/seconds_dt) * avg_deadt)\n",
    "    return counts_c\n",
    "\n",
    "def altitude_logarithmic(P, T, QFF):\n",
    "    # ftp://ftp.nist.gov/pub/physics/lunarproject/References/Atmospheric_Properties/Pressure%20to%20Altitude%20Conversion%20%232.pdf\n",
    "    P_0 = np.array(np.ones_like(P) * QFF, dtype='float64')\n",
    "    T_0 = 288.150\n",
    "    alt = (10**(np.log10(P/P_0)/5.2558797)- 1)/(-6.8755856*10**(-6))\n",
    "    alt = alt * (T[0]+273.15)/T_0\n",
    "    alt = alt * 0.3048  # Convert to meters\n",
    "    return alt\n",
    "\n",
    "def altitude_hypsometric(T, P):\n",
    "    N = 2\n",
    "    R = 287.058\n",
    "    g = 9.80665\n",
    "    \n",
    "    dz = np.zeros_like(T)\n",
    "    \n",
    "    for i in range(1, len(T)):\n",
    "        T_ave = ((T[i] + T[i-1]) / 2.0) + 273.15\n",
    "        P1 = P[i-1]\n",
    "        P2 = P[i]\n",
    "        \n",
    "        dz[i] = (R * T_ave / g) * np.log(P1/P2)\n",
    "    \n",
    "    alt = np.cumsum(dz)\n",
    "    \n",
    "    return alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STAR log file began at 12:59:55 EST on 5-13-2018\n",
    "\n",
    "try:\n",
    "    if os.path.isfile(star_file):\n",
    "        df_star = pd.read_csv(star_file, index_col='Elapsed')\n",
    "        \n",
    "except:\n",
    "    # The CSV file has a header row\n",
    "    header = True\n",
    "\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    with open(counts_file, 'rb') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in csvreader:\n",
    "            if (header):\n",
    "                indexName  = row[0]   # assume that first column label is the index\n",
    "                df_columns = row[1:]  # second through final header become DataFrame column names\n",
    "                df_star = pd.DataFrame(columns=df_columns)\n",
    "                header = False\n",
    "                next\n",
    "            else:\n",
    "                for ii in range(len(row)):\n",
    "                    if is_number(row[ii]):\n",
    "                        row[ii] = cvt_number(row[ii])\n",
    "                    df_star = df_star.append(pd.DataFrame([row[1:]], columns=df_columns, index=[row[0]]), ignore_index=False)\n",
    "\n",
    "    df_star.to_csv(star_file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Telemetry log file began at 13:40:03 EST on 5-13-2018 (17:40:03Z)\n",
    "# This is 0:40:08 after STAR began logging\n",
    "\n",
    "try:\n",
    "    if os.path.isfile(tel_file):\n",
    "        with open(tel_file, 'r') as telfile:\n",
    "            indexName = ['Elapsed']\n",
    "            df_columns = ['Timestamp', 'Lat', 'Long', 'Elev']\n",
    "            df_tel = pd.DataFrame(columns=df_columns)\n",
    "            for row in telfile:\n",
    "                try:\n",
    "                    m = re.match(pat, row)\n",
    "                    if (m):\n",
    "                        dt = datetime.datetime(2018, 5, 13, (int(m.group(1))-4), int(m.group(2)), int(m.group(3)))\n",
    "                        hh = int(m.group(1)) - 17  # Telemetry was in Zulu time\n",
    "                        mm = int(m.group(2)) - 40 + 40  # To sync with STAR data\n",
    "                        ss = int(m.group(3)) - 3 + 8    # To sync with STAR data\n",
    "                        elapsed_time = hh * 3600 + mm * 60 + ss\n",
    "                        df_tel = df_tel.append(pd.DataFrame([[dt, float(m.group(4)), -1*float(m.group(5)), float(m.group(6))]], columns=df_columns, index=[elapsed_time]), ignore_index=False)\n",
    "                    else:\n",
    "                        print row\n",
    "                except:\n",
    "                    print \"Exception.\"\n",
    "    else:\n",
    "        print \"File not found!\"\n",
    "except:\n",
    "    print \"File IO error.\"\n",
    "\n",
    "# Remove any duplicates\n",
    "df_tel = df_tel[~df_tel.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid GPS data begins at 13:40:03 EST on 5-13-2018 (17:40:03Z)\n",
    "# This is 0:40:08 after STAR began logging\n",
    "\n",
    "# Conversion from Degrees Minutes.m to Decimal Degrees\n",
    "# https://www.directionsmag.com/site/latlong-converter/\n",
    "\n",
    "try:\n",
    "    if os.path.isfile(gps_file):\n",
    "        with open(gps_file, 'r') as gpsfile:\n",
    "            indexName = ['Elapsed']\n",
    "            df_columns = ['Datetime', 'Lat', 'Long', 'Elev', 'Lock', 'Sats', 'HDOP']\n",
    "            df_gps = pd.DataFrame(columns=df_columns)\n",
    "            for row in gpsfile:\n",
    "                \n",
    "                # Try to match a regex\n",
    "                try:\n",
    "                    m = re.match(pat1, row)\n",
    "                    \n",
    "                    # Match\n",
    "                    if (m):\n",
    "                        \n",
    "                        #&utc_time, &latitude, &ns, &longitude, &ew, &lock, &satellites, &hdop, &altitude, &units\n",
    "\n",
    "                        # Only include data from after 17Z\n",
    "                        if (int(m.group(1)) >= 17):\n",
    "                            #print m.group(1), m.group(2), m.group(3)\n",
    "                            dt = datetime.datetime(2018, 5, 13, (int(m.group(1))-4), int(m.group(2)), int(m.group(3)))\n",
    "                            hh = int(m.group(1)) - 17       # Telemetry was in Zulu time\n",
    "                            mm = int(m.group(2)) - 40 + 40  # To sync with STAR data\n",
    "                            ss = int(m.group(3)) - 3 + 8    # To sync with STAR data\n",
    "                            elapsed_time = hh * 3600 + mm * 60 + ss\n",
    "                            \n",
    "                            lat = int(m.group(4)) + float(m.group(5)) / 60.0\n",
    "                            lon = -1 * (int(m.group(6)) + float(m.group(7)) / 60.0)\n",
    "                            \n",
    "                            lock = int(m.group(8))\n",
    "                            sats = int(m.group(9))\n",
    "                            hdop = float(m.group(10))\n",
    "                            \n",
    "                            elev = float(m.group(11))\n",
    "\n",
    "                            df_gps = df_gps.append(pd.DataFrame([[dt, lat, lon, elev, lock, sats, hdop]], columns=df_columns, index=[elapsed_time]), ignore_index=False)\n",
    "                    else:\n",
    "                        next\n",
    "                        #print row\n",
    "                except:\n",
    "                    print \"Exception.\"\n",
    "    else:\n",
    "        print \"File not found!\"\n",
    "except:\n",
    "    print \"File IO error.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "    Directory: C:\\Users\\c_nic\\Documents\\ASTR 202\\pits\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mode                LastWriteTime         Length Name                                                                  \n",
      "\n",
      "----                -------------         ------ ----                                                                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PITS images began at 13:40:31 EST on 5-13-2018\n",
    "# This is 0:40:36 after STAR began logging\n",
    "\n",
    "try:\n",
    "    if os.path.isfile(pits_file):\n",
    "        with open(pits_file, 'r') as pitsfile:\n",
    "            indexName = ['Elapsed']\n",
    "            df_columns = ['PITS Image']\n",
    "            df_pits = pd.DataFrame(columns=df_columns)\n",
    "            for row in pitsfile:\n",
    "                \n",
    "                # Try to match a regex\n",
    "                try:\n",
    "                    m = re.match(pat2, row)\n",
    "                    \n",
    "                    # Match\n",
    "                    if (m):\n",
    "                        hh = int(m.group(2)) - 13      # To sync with STAR data\n",
    "                        mm = int(m.group(3)) - 40 + 40 # To sync with STAR data\n",
    "                        ss = int(m.group(4)) - 31 + 36 # To sync with STAR data\n",
    "                        elapsed_time = hh * 3600 + mm * 60 + ss\n",
    "                        \n",
    "                        df_pits = df_pits.append(pd.DataFrame([m.group(1)], columns=df_columns, index=[elapsed_time]), ignore_index=False)\n",
    "                    else:\n",
    "                        print row\n",
    "                except:\n",
    "                    next\n",
    "    else:\n",
    "        print \"File not found!\"\n",
    "except:\n",
    "    print \"File IO error.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "    Directory: C:\\Users\\c_nic\\Documents\\ASTR 202\\msi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mode                LastWriteTime         Length Name                                                                  \n",
      "\n",
      "----                -------------         ------ ----                                                                  \n",
      "\n",
      "No seed found.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MSI images began at 13:39:34 EST on 5-13-2018\n",
    "# This is 0:39:39 after STAR began logging\n",
    "\n",
    "try:\n",
    "    if os.path.isfile(msi_file):\n",
    "        with open(msi_file, 'r') as msifile:\n",
    "            indexName = ['Elapsed']\n",
    "            df_columns = ['MSI Image', 'MSI IR Image']\n",
    "            df_msi = pd.DataFrame(columns=df_columns)\n",
    "            for row in msifile:\n",
    "                try:\n",
    "                    # Try to match a regex\n",
    "                    m = re.match(pat3, row)\n",
    "                    \n",
    "                    # Match\n",
    "                    if (m):\n",
    "                        cam0 = m.group(1).replace(\"cam1\", \"cam0\")\n",
    "                        cam1 = m.group(1).replace(\"cam0\", \"cam1\")\n",
    "                        \n",
    "                        # There are three randomly generated \"seeds\" appended to the filenames\n",
    "                        # Each \"seed\" corresponds to a reboot\n",
    "                        seed = int(m.group(5))\n",
    "                        \n",
    "                        try:\n",
    "                            which_seed = int(np.where(msi_seeds == seed)[0][0])\n",
    "\n",
    "                            # Normal operation\n",
    "                            if (which_seed == 0):\n",
    "                                # MSI images began at 13:39:34\n",
    "                                #print m.group(2), m.group(3), m.group(4)\n",
    "                                hh = int(m.group(2)) - 13\n",
    "                                mm = int(m.group(3)) - 39 + 39  # To sync with STAR data\n",
    "                                ss = int(m.group(4)) - 34 + 39  # To sync with STAR data\n",
    "                                elapsed_time = hh * 3600 + mm * 60 + ss\n",
    "                                \n",
    "                                df_msi = df_msi.append(pd.DataFrame([[cam0, cam1]], columns=df_columns, index=[elapsed_time]), ignore_index=False)\n",
    "                            \n",
    "                            # First reboot\n",
    "                            elif (which_seed == 1):\n",
    "                                # Reboot happened around 15:07:35 and lasted 4.5s\n",
    "                                # This was determined using syslog and the last image timstamp\n",
    "                                # Time was reset to 12:00:00\n",
    "                                # This is 1:28:01 after MSI began logging\n",
    "\n",
    "                                #print m.group(2), m.group(3), m.group(4)\n",
    "                                hh = int(m.group(2)) - 12 + 0  + 1\n",
    "                                mm = int(m.group(3)) - 0  + 39 + 28     # To sync with STAR data\n",
    "                                ss = int(m.group(4)) - 0  + 39 + 1 + 5  # To sync with STAR data\n",
    "                                elapsed_time = hh * 3600 + mm * 60 + ss\n",
    "\n",
    "                                df_msi = df_msi.append(pd.DataFrame([[cam0, cam1]], columns=df_columns, index=[elapsed_time]), ignore_index=False)\n",
    "                            \n",
    "                            # Second reboot\n",
    "                            elif (which_seed == 2):\n",
    "                                # Reboot happened around 12:47:00 and lasted 3.3s\n",
    "                                # This was determined using syslog and the last image timstamp\n",
    "                                # Time was reset to 12:00:00\n",
    "                                # This is 2:15:01 after MSI began logging\n",
    "                                \n",
    "                                #print m.group(2), m.group(3), m.group(4)\n",
    "\n",
    "                                hh = int(m.group(2)) - 12 + 0  + 1  + 0\n",
    "                                mm = int(m.group(3)) - 0  + 39 + 28 + 47\n",
    "                                ss = int(m.group(4)) - 0  + 39 + 1  + 0  + 8\n",
    "                                elapsed_time = hh * 3600 + mm * 60 + ss\n",
    "\n",
    "                                df_msi = df_msi.append(pd.DataFrame([[cam0, cam1]], columns=df_columns, index=[elapsed_time]), ignore_index=False)\n",
    "                            else:\n",
    "                                print m.group(1)\n",
    "                        except:\n",
    "                            print \"No seed found.\"\n",
    "                    else:\n",
    "                        print row\n",
    "                except:\n",
    "                    next\n",
    "    else:\n",
    "        print \"File not found!\"\n",
    "except:\n",
    "    print \"File IO error.\"\n",
    "    \n",
    "# Sort the dataframe index to get the elapsed times in order\n",
    "df_msi = df_msi.sort_index()\n",
    "\n",
    "# Remove any duplicates\n",
    "df_msi = df_msi[~df_msi.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Counts   T (Raw)     T1 (C)   P (Raw)    P1 (mbar)    P2 (mbar)  \\\n",
      "Elapsed                                                                        \n",
      "792305.504       -1   8424522  29.624726   6760918  1009.786643  1009.786643   \n",
      "792306.504       -1   8424604  29.627446   6760768  1009.733632  1009.733632   \n",
      "792307.504       -1   8424642  29.628707   6760756  1009.731755  1009.731755   \n",
      "792308.504       -1   8424664  29.629437   6760868  1009.777593  1009.777593   \n",
      "\n",
      "             Altitude (m)   Dead Time (s)   Dead Time Counts  \n",
      "Elapsed                                                       \n",
      "792305.504      97.012924             0.0                 -1  \n",
      "792306.504      97.467407             0.0                 -1  \n",
      "792307.504      97.483696             0.0                 -1  \n",
      "792308.504      97.091202             0.0                 -1  \n",
      "               Timestamp       Lat      Long   Elev\n",
      "2408 2018-05-13 13:40:03  43.08361 -73.76945  105.0\n",
      "2409 2018-05-13 13:40:04  43.08361 -73.76946  105.0\n",
      "2414 2018-05-13 13:40:09  43.08362 -73.76946  102.0\n",
      "2428 2018-05-13 13:40:23  43.08362 -73.76949  110.0\n",
      "                Datetime        Lat       Long   Elev  Lock  Sats  HDOP\n",
      "2408 2018-05-13 13:40:03  43.083610 -73.769451  105.3   1.0   4.0  5.23\n",
      "2409 2018-05-13 13:40:04  43.083614 -73.769454  105.4   1.0   4.0  5.23\n",
      "2413 2018-05-13 13:40:08  43.083589 -73.769474  103.5   1.0   4.0  5.23\n",
      "2414 2018-05-13 13:40:09  43.083617 -73.769466  102.8   1.0   4.0  5.23\n",
      "            PITS Image\n",
      "2436  13_40_31-rot.jpg\n",
      "2496  13_41_31-rot.jpg\n",
      "2556  13_42_31-rot.jpg\n",
      "2616  13_43_31-rot.jpg\n",
      "                                 MSI Image  \\\n",
      "2379  2018-05-13_133934-391900854-cam0.jpg   \n",
      "2439  2018-05-13_134034-391900854-cam0.jpg   \n",
      "2499  2018-05-13_134134-391900854-cam0.jpg   \n",
      "2559  2018-05-13_134234-391900854-cam0.jpg   \n",
      "\n",
      "                              MSI IR Image  \n",
      "2379  2018-05-13_133934-391900854-cam1.jpg  \n",
      "2439  2018-05-13_134034-391900854-cam1.jpg  \n",
      "2499  2018-05-13_134134-391900854-cam1.jpg  \n",
      "2559  2018-05-13_134234-391900854-cam1.jpg  \n"
     ]
    }
   ],
   "source": [
    "print df_star.head(4)\n",
    "print df_tel.head(4)\n",
    "print df_gps.head(4)\n",
    "print df_pits.head(4)\n",
    "print df_msi.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-13 13:40:03 1526233203.0\n",
      "0.0 2408 2436 2379\n"
     ]
    }
   ],
   "source": [
    "# STAR data\n",
    "seconds = df_star.index.values[:]\n",
    "\n",
    "# Turn seconds into elapsed time from zero\n",
    "elapsed = seconds - seconds[0]\n",
    "\n",
    "# GPS data\n",
    "tel_seconds = df_gps.index.values[:]\n",
    "tel_dt = pd.to_datetime(np.array(df_gps.iloc[:,0].values[:]))\n",
    "tel_ts = [time.mktime(i.timetuple()) for i in tel_dt]\n",
    "\n",
    "# PITS image data\n",
    "pits_seconds = df_pits.index.values[:]\n",
    "pits_fn = np.array(df_pits.iloc[:,0].values[:], dtype='str')\n",
    "\n",
    "# MSI image data\n",
    "msi_seconds = df_msi.index.values[:]\n",
    "msi_fn = np.array(df_msi.iloc[:,0].values[:], dtype='str')\n",
    "msi_ir = np.array(df_msi.iloc[:,1].values[:], dtype='str')\n",
    "\n",
    "print tel_dt[0], tel_ts[0]\n",
    "print elapsed[0], tel_seconds[0], pits_seconds[0], msi_seconds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT USE\n",
    "\n",
    "# Attempt to align the dataframes\n",
    "# Telemetry: altitude max at 9927\n",
    "# STAR: altitude max at 804614.504000\n",
    "# STAR: log started at 12:59:55pm\n",
    "\n",
    "#star_max = elapsed[np.where(P2==np.min(P2))][0]\n",
    "#tel_max = tel_seconds[np.where(tel_alt==np.max(tel_alt))][0]\n",
    "\n",
    "#print \"STAR maximum altitude at: \", star_max\n",
    "#print \"Telemetry maximum altitude at: \", tel_max\n",
    "\n",
    "# Align the two data sets\n",
    "#tel_seconds2 = tel_seconds + (star_max - tel_max)  #+ 27\n",
    "#pits_seconds2 = pits_seconds + (star_max - tel_max) + 27\n",
    "#msi_seconds2 = msi_seconds + (star_max - tel_max) + 27\n",
    "\n",
    "#print \"Elapsed time offset: \", tel_seconds2[0], pits_seconds2[0], msi_seconds2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create interpolation functions for the missing data\n",
    "alt_func = interp1d(tel_seconds, df_gps.iloc[:,3].values[:], fill_value='extrapolate')\n",
    "lat_func = interp1d(tel_seconds, df_gps.iloc[:,1].values[:], fill_value='extrapolate')\n",
    "long_func = interp1d(tel_seconds, df_gps.iloc[:,2].values[:], fill_value='extrapolate')\n",
    "ts_func = interp1d(tel_seconds, tel_ts, fill_value='extrapolate')\n",
    "\n",
    "# Create an integer version of the seconds array\n",
    "seconds2 = np.array(elapsed, dtype='int')\n",
    "\n",
    "# Create arrays\n",
    "dt2 = np.empty(len(seconds2), dtype='datetime64[s]')\n",
    "\n",
    "# GPS arrays\n",
    "lat2 = np.ones_like(seconds2) * np.nan\n",
    "long2 = np.ones_like(seconds2) * np.nan\n",
    "alt2 = np.ones_like(seconds2) * np.nan\n",
    "lock2 = np.ones_like(seconds2) * np.nan\n",
    "sats2 = np.ones_like(seconds2) * np.nan\n",
    "hdop2 = np.ones_like(seconds2) * np.nan\n",
    "\n",
    "# PITS arrays\n",
    "fn2 = np.empty(len(seconds2), dtype='object')\n",
    "\n",
    "# MSI arrays\n",
    "msi_fn1 = np.empty(len(seconds2), dtype='object')\n",
    "msi_fn2 = np.empty(len(seconds2), dtype='object')\n",
    "\n",
    "# Create a binary value to indicate interpolated data\n",
    "interp = np.zeros_like(seconds2)\n",
    "\n",
    "# Set up timestamp extrapolation\n",
    "ts_last = 0\n",
    "ts_td = np.timedelta64(datetime.timedelta(seconds=1))\n",
    "\n",
    "# Iterate all the seconds\n",
    "for i in range(len(seconds2)):\n",
    "    cur_sec = seconds2[i]\n",
    "    \n",
    "    # Try to locate data in the telemetry arrays\n",
    "    try:\n",
    "        tel_loc = np.where(tel_seconds == cur_sec)[0][0]\n",
    "        dt2[i] = np.datetime64(tel_dt[tel_loc])\n",
    "        alt2[i] = df_gps.iloc[tel_loc, 3]\n",
    "        lat2[i] = df_gps.iloc[tel_loc, 1]\n",
    "        long2[i] = df_gps.iloc[tel_loc, 2]\n",
    "        lock2[i] = df_gps.iloc[tel_loc, 4]\n",
    "        sats2[i] = df_gps.iloc[tel_loc, 5]\n",
    "        hdop2[i] = df_gps.iloc[tel_loc, 6]\n",
    "        interp[i] = int(0)\n",
    "\n",
    "    # Interpolate missing data\n",
    "    except:\n",
    "        \n",
    "        # Only interpolate during the range of valid GPS data\n",
    "        if ((cur_sec >= tel_seconds[0]) and (cur_sec <= tel_seconds[-1])):\n",
    "            alt2[i] = alt_func(cur_sec)\n",
    "            lat2[i] = lat_func(cur_sec)\n",
    "            long2[i] = long_func(cur_sec)\n",
    "            lock2[i] = np.nan\n",
    "            sats2[i] = np.nan\n",
    "            hdop2[i] = np.nan\n",
    "            interp[i] = int(1)\n",
    "            \n",
    "        # Otherwise, set everything to NaN\n",
    "        else:\n",
    "            alt2[i] = np.nan\n",
    "            lat2[i] = np.nan\n",
    "            long2[i] = np.nan\n",
    "            lock2[i] = np.nan\n",
    "            sats2[i] = np.nan\n",
    "            hdop2[i] = np.nan\n",
    "            interp[i] = int(0)\n",
    "\n",
    "        # Try to get the last valid timestamp\n",
    "        try:\n",
    "            dt2[i] = np.datetime64(datetime.datetime.fromtimestamp(ts_func(cur_sec).tolist()))\n",
    "            \n",
    "        # Otherwise, extrapolate it by adding a second to last known timestamp\n",
    "        except:\n",
    "            dt2[i] = ts_last + ts_td\n",
    "\n",
    "    # Is there a PITS image associated with this timestamp?\n",
    "    try:\n",
    "        pits_loc = np.where(pits_seconds == cur_sec)[0][0]\n",
    "        fn2[i] = str(pits_fn[pits_loc])\n",
    "        \n",
    "    # No.\n",
    "    except:\n",
    "        fn2[i] = ''\n",
    "\n",
    "    # Are there a MSI images associated with this timestamp?\n",
    "    try:\n",
    "        msi_loc = np.where(msi_seconds == cur_sec)[0][0]\n",
    "        msi_fn1[i] = str(msi_fn[msi_loc])\n",
    "        msi_fn2[i] = str(msi_ir[msi_loc])\n",
    "        \n",
    "    # No.\n",
    "    except:\n",
    "        msi_fn1[i] = ''\n",
    "        msi_fn2[i] = ''\n",
    "\n",
    "    # Capture the last timestamp\n",
    "    ts_last = dt2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with all the data\n",
    "df_columns = ['Date Time', 'Elapsed', 'Counts', 'T [Raw]', 'T1 [C]', 'P [Raw]', 'P1 [mb]', 'P2 [mb]', 'Est. Alt. [m]', 'Dead Time [s]', 'Dead Time Counts', 'GPS Lat [DD]', 'GPS Long [DD]', 'GPS Elev [m]', 'GPS Lock', 'GPS Sats', 'GPS HDOP', 'Interp?', 'PITS Image', 'MSI Image', 'MSI IR Image']\n",
    "\n",
    "# Zip all the data together\n",
    "z = zip(dt2, seconds, df_star.iloc[:,0].values[:], df_star.iloc[:,1].values[:], df_star.iloc[:,2].values[:], df_star.iloc[:,3].values[:], \\\n",
    "        df_star.iloc[:,4].values[:], df_star.iloc[:,5].values[:], df_star.iloc[:,6].values[:], \\\n",
    "        df_star.iloc[:,7].values[:], df_star.iloc[:,8].values[:], lat2, long2, alt2, lock2, sats2, hdop2, interp, \\\n",
    "        fn2, msi_fn1, msi_fn2)\n",
    "\n",
    "# Populate the dataframe\n",
    "df_final = pd.DataFrame(data=z, columns=df_columns, index=seconds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date Time     Elapsed  Counts  T [Raw]     T1 [C]  P [Raw]  \\\n",
      "0 2018-05-13 12:59:55  792305.504      -1  8424522  29.624726  6760918   \n",
      "1 2018-05-13 12:59:56  792306.504      -1  8424604  29.627446  6760768   \n",
      "2 2018-05-13 12:59:57  792307.504      -1  8424642  29.628707  6760756   \n",
      "3 2018-05-13 12:59:58  792308.504      -1  8424664  29.629437  6760868   \n",
      "\n",
      "       P1 [mb]      P2 [mb]  Est. Alt. [m]  Dead Time [s]     ...       \\\n",
      "0  1009.786643  1009.786643      97.012924            0.0     ...        \n",
      "1  1009.733632  1009.733632      97.467407            0.0     ...        \n",
      "2  1009.731755  1009.731755      97.483696            0.0     ...        \n",
      "3  1009.777593  1009.777593      97.091202            0.0     ...        \n",
      "\n",
      "   GPS Lat [DD]  GPS Long [DD]  GPS Elev [m]  GPS Lock  GPS Sats  GPS HDOP  \\\n",
      "0           NaN            NaN           NaN       NaN       NaN       NaN   \n",
      "1           NaN            NaN           NaN       NaN       NaN       NaN   \n",
      "2           NaN            NaN           NaN       NaN       NaN       NaN   \n",
      "3           NaN            NaN           NaN       NaN       NaN       NaN   \n",
      "\n",
      "   Interp?  PITS Image MSI Image MSI IR Image  \n",
      "0        0                                     \n",
      "1        0                                     \n",
      "2        0                                     \n",
      "3        0                                     \n",
      "\n",
      "[4 rows x 21 columns]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c2157194e124>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoined_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print df_final.head(4)\n",
    "\n",
    "# Save the dataframe to file\n",
    "df_final.to_csv(joined_file, sep=',', encoding='ascii')\n",
    "\n",
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-dfa145711c91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdf_tel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Free up some RAM\n",
    "del [[df_star, df_gps, df_pits, df_msi, df_tel]]\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "df_star=pd.DataFrame()\n",
    "df_gps=pd.DataFrame()\n",
    "df_pits=pd.DataFrame()\n",
    "df_msi=pd.DataFrame()\n",
    "df_tel=pd.DataFrame()\n",
    "\n",
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create KML file from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-5eee52b4367b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kml22gx.xsd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dt = 0\n",
    "lat = 0\n",
    "lon = 0\n",
    "elev = 0\n",
    "pits = ''\n",
    "msi1 = ''\n",
    "msi2 = ''\n",
    "\n",
    "# TO DO:\n",
    "# Style the Placemerks and extrusion lines\n",
    "# Put lat/long/elev in description\n",
    "\n",
    "doc = KML.Document()\n",
    "fld = KML.Folder()\n",
    "tour = GX.Tour(KML.name(\"Fly with SOAR!\"))\n",
    "playlist = GX.Playlist()\n",
    "\n",
    "doc.append(KML.name(\"Flight of SOAR, May 13, 2018\"))\n",
    "\n",
    "ps = KML.Style(KML.IconStyle(KML.scale(1.0), \\\n",
    "                          KML.Icon(KML.href(\"http://maps.google.com/mapfiles/kml/pal4/icon46.png\"),), \\\n",
    "                          id=\"camera\"), \\\n",
    "            id=\"pits\")\n",
    "\n",
    "doc.append(ps)\n",
    "\n",
    "ms1 = KML.Style(KML.IconStyle(KML.scale(1.0), \\\n",
    "                          KML.Icon(KML.href(\"http://maps.google.com/mapfiles/kml/paddle/wht-blank.png\"),), \\\n",
    "                          id=\"visible\"), \\\n",
    "            id=\"ms1\")\n",
    "\n",
    "doc.append(ms1)\n",
    "\n",
    "ms2 = KML.Style(KML.IconStyle(KML.scale(1.0), \\\n",
    "                          KML.Icon(KML.href(\"http://maps.google.com/mapfiles/kml/paddle/pink-blank.png\"),), \\\n",
    "                          id=\"infrared\"), \\\n",
    "            id=\"ms2\")\n",
    "\n",
    "doc.append(ms2)\n",
    "\n",
    "lastlat = 0\n",
    "lastlon = 0\n",
    "bearing = 0\n",
    "\n",
    "for index, row in df_final.iterrows():\n",
    "    dt = np.datetime64(row['Date Time'])\n",
    "    lat = row['GPS Lat [DD]']\n",
    "    lon = row['GPS Long [DD]']\n",
    "    elev = row['GPS Elev [m]']\n",
    "    \n",
    "    if (not np.isnan(lat) and not np.isnan(lon) and not np.isnan(elev)):\n",
    "        pits = row['PITS Image']\n",
    "        msi1 = row['MSI Image']\n",
    "        msi2 = row['MSI IR Image']\n",
    "        \n",
    "        if (pits):\n",
    "            pm1 = KML.Placemark(KML.name(pd.to_datetime(str(dt)).strftime(\"%H:%M:%S\")), \\\n",
    "                                KML.styleUrl(\"#pits\"), \\\n",
    "                                KML.Point(KML.extrude(True), KML.altitudeMode('absolute'), \\\n",
    "                                          KML.coordinates(\"{0},{1},{2}\".format(lon,lat,elev))), \\\n",
    "                                KML.Snippet('\"' + pits + '\"'), \\\n",
    "                                KML.description(\"Latitude: {0}<br>Longitude: {1}<br>Elevation: {2}<br><img src='https://soarwellesley2018.files.wordpress.com/2018/05/{3}' width='640' />\".format(lon,lat,elev,pits)))\n",
    "            \n",
    "            fld.append(pm1)\n",
    "            \n",
    "            # https://www.movable-type.co.uk/scripts/latlong.html\n",
    "            bearing = (np.angle(np.arctan2(np.cos(lastlat)*np.sin(lat)-np.sin(lastlat)*np.cos(lat)*np.cos(lon-lastlon), \\\n",
    "                                          np.sin(lon-lastlon)*np.cos(lat)), deg=True) + 360) % 360\n",
    "\n",
    "            fly = GX.FlyTo( \\\n",
    "                     GX.duration(3), \\\n",
    "                     GX.flyToMode(\"smooth\"), \\\n",
    "                     KML.Camera( \\\n",
    "                                KML.longitude(lon), \\\n",
    "                                KML.latitude(lat), \\\n",
    "                                KML.altitude(elev + 1000), \\\n",
    "                                KML.heading(bearing), \\\n",
    "                                KML.tilt(5.0), \\\n",
    "                               ) \\\n",
    "                    )\n",
    "            playlist.append(fly)\n",
    "            \n",
    "            wait = GX.Wait(GX.duration(0))\n",
    "            \n",
    "            playlist.append(wait)\n",
    "        \n",
    "        \n",
    "        if (msi1):\n",
    "            pm1 = KML.Placemark(KML.name(pd.to_datetime(str(dt)).strftime(\"%H:%M:%S\")), \\\n",
    "                                KML.styleUrl(\"#ms1\"), \\\n",
    "                                KML.Point(KML.extrude(True), KML.altitudeMode('absolute'), \\\n",
    "                                          KML.coordinates(\"{0},{1},{2}\".format(lon,lat,elev))), \\\n",
    "                                KML.Snippet('\"' + msi1 + '\"'), \\\n",
    "                                KML.description(\"Latitude: {0}<br>Longitude: {1}<br>Elevation: {2}<br><img src='https://soarwellesley2018.files.wordpress.com/2018/05/{3}' width='640' />\".format(lon,lat,elev,msi1)))\n",
    "            \n",
    "            fld.append(pm1)\n",
    "            \n",
    "        if (msi2):\n",
    "            pm1 = KML.Placemark(KML.name(pd.to_datetime(str(dt)).strftime(\"%H:%M:%S\")), \\\n",
    "                                KML.styleUrl(\"#ms2\"), \\\n",
    "                                KML.Point(KML.extrude(True), KML.altitudeMode('absolute'), \\\n",
    "                                          KML.coordinates(\"{0},{1},{2}\".format(lon,lat,elev))), \\\n",
    "                                KML.Snippet('\"' + msi2 + '\"'), \\\n",
    "                                KML.description(\"Latitude: {0}<br>Longitude: {1}<br>Elevation: {2}<br><img src='https://soarwellesley2018.files.wordpress.com/2018/05/{3}' width='640' />\".format(lon,lat,elev,msi2)))\n",
    "            \n",
    "            fld.append(pm1)\n",
    "            \n",
    "        lastlat = lat\n",
    "        lastlon = lon\n",
    "\n",
    "doc.append(fld)\n",
    "\n",
    "tour.append(playlist)\n",
    "\n",
    "doc.append(tour)\n",
    "            \n",
    "outfile = file('soar-images.kml','w')\n",
    "outfile.write(etree.tostring(doc, pretty_print=True))\n",
    "outfile.close()\n",
    "\n",
    "assert Schema('kml22gx.xsd').validate(fld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some data to use for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seconds = np.array(df_final.index)\n",
    "mins = seconds / 60.0\n",
    "P2 = np.array(df_final['P2 [mb]'])\n",
    "T = np.array(df_final['T1 [C]'])\n",
    "\n",
    "alt_star = np.array(df_final['Est. Alt. [m]'])\n",
    "alt_gps = np.array(df_final['GPS Elev [m]'])\n",
    "\n",
    "dt = np.gradient(seconds)\n",
    "dx = np.gradient(alt_gps)\n",
    "\n",
    "elapsed = np.array(df_final['Elapsed'])\n",
    "elapsed = elapsed - elapsed[0]\n",
    "\n",
    "# Where are the counts >= 0?\n",
    "counts_min = loc_min_counts(df_final['Dead Time Counts'], 0)\n",
    "\n",
    "# Time intervals\n",
    "star_dt = np.gradient(elapsed)\n",
    "\n",
    "# Average dead time\n",
    "avg_deadt = avg_deadtime(df_final['Dead Time [s]'], df_final['Dead Time Counts'])\n",
    "\n",
    "# Dead time correction\n",
    "counts_c = deadt_correction(star_dt, df_final['Dead Time Counts'], avg_deadt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts and Pressure vs Time (Wes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts_smoothed = savgol_filter(counts_c, 51, 1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(mins[counts_min], counts_smoothed[counts_min], 'r.')\n",
    "ax1.tick_params('y', colors='r')\n",
    "ax1.set_xlabel(\"Time [min]\")\n",
    "ax1.set_ylabel(\"Counts\")\n",
    "ax1.set_ylim([0, 25])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(mins[counts_min], P2[counts_min], 'b-', linewidth=3)\n",
    "ax2.tick_params('y', colors='b')\n",
    "ax2.set_ylabel(\"Pressure [mb]\")\n",
    "\n",
    "plt.xlim([mins[counts_min][0], mins[counts_min][-1]])\n",
    "\n",
    "plt.savefig(\"%s/counts_pressure_time.png\" % (data_dir), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts vs Pressure (Wes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.semilogx(P2, counts_smoothed, 'r.')\n",
    "plt.xlabel(\"Pressure [mb]\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "plt.xlim([np.min(P2), np.max(P2)])\n",
    "\n",
    "plt.savefig(\"%s/counts_pressure.png\" % (data_dir), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elevation vs Time (Wes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "QFF = 1022  # https://weather.us/observations/new-york/pressure-qff/20180513-1500z.html\n",
    "alt_log = altitude_logarithmic(P2, T, QFF)\n",
    "#alt_h = altitude_hypsometric(T, P2)\n",
    "\n",
    "print np.max(alt_log), np.max(alt_gps)\n",
    "\n",
    "plt.plot(mins, alt_star/1000.0, 'r--', linewidth=3, label=\"Hypsometric approximation\")\n",
    "plt.plot(mins, alt_log/1000.0, 'b--', linewidth=3, label=\"Logarithmic approximation\")\n",
    "plt.plot(mins, alt_gps/1000.0, 'g-', linewidth=3, label=\"GPS [Measured and Interpolated]\")\n",
    "plt.xlabel(\"Time [min]\")\n",
    "plt.ylabel(\"Elevation [km]\")\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.xlim([50, 275])\n",
    "\n",
    "plt.savefig(\"%s/elevation_time.png\" % (data_dir), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts and Elevation vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "counts_smoothed = savgol_filter(counts_c, 51, 1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(mins, counts_smoothed, 'r.', label=\"Counts\")\n",
    "ax1.tick_params('y', colors='r')\n",
    "ax1.set_xlabel(\"Time [min]\")\n",
    "ax1.set_ylabel(\"Counts\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(mins, alt_gps/1000.0, 'b-', linewidth=3)\n",
    "ax2.tick_params('y', colors='b')\n",
    "ax2.set_ylabel(\"Elevation [km]\")\n",
    "\n",
    "plt.xlim([78, 248])\n",
    "ax1.set_ylim([0, 30])\n",
    "ax2.set_ylim([0, 30])\n",
    "\n",
    "plt.savefig(\"%s/counts_elevation_time.png\" % (data_dir), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velocity (Wes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "v = np.array(dx/dt, dtype='float64')\n",
    "v_smooth = savgol_filter(v, 3, 1)\n",
    "\n",
    "plt.ylabel(\"Velocity [m/s]\")\n",
    "plt.xlabel(\"Time [min]\")\n",
    "\n",
    "x = np.linspace(np.min(mins), np.max(mins), 1000)\n",
    "\n",
    "plt.plot(mins, v_smooth, linewidth=3)\n",
    "plt.xlim([75, 275])\n",
    "\n",
    "plt.savefig(\"%s/velocity_time.png\" % (data_dir), dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velocity (Ascent Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.ylabel(\"Velocity [m/s]\")\n",
    "plt.xlabel(\"Time [min]\")\n",
    "\n",
    "m = list(np.where(mins <= 205.0)[0])\n",
    "n = list(np.where(mins >= 77.0)[0])\n",
    "o = np.array(list(set(m) & set(n)))\n",
    "\n",
    "mean_v = np.nanmean(v_smooth[o])\n",
    "\n",
    "x = np.linspace(np.min(mins[o]), np.max(mins[o]), 1000)\n",
    "\n",
    "plt.plot(mins[o], v_smooth[o], linewidth=3, label=\"Mean Ascent Velocity: %.2f m/s\" % mean_v)\n",
    "plt.xlim([np.min(x), np.max(x)])\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.savefig(\"%s/velocity_ascent.png\" % (data_dir), dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velocity (Descent Only, Wes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "plt.ylabel(\"Velocity [m/s]\")\n",
    "plt.xlabel(\"Time [min]\")\n",
    "\n",
    "m = list(np.where(mins >= 206.0)[0])\n",
    "n = list(np.where(mins <= 250.0)[0])\n",
    "o = np.array(list(set(m) & set(n)))\n",
    "\n",
    "mean_v = np.nanmean(v_smooth[o])\n",
    "\n",
    "x = np.linspace(np.min(mins[o]), np.max(mins[o]), 1000)\n",
    "\n",
    "plt.plot(mins[o], v_smooth[o], linewidth=3, label=\"Mean Descent Velocity: %.2f m/s\" % mean_v)\n",
    "plt.xlim([np.min(x), np.max(x)])\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.savefig(\"%s/velocity_descent.png\" % (data_dir), dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Temperature vs Time (Wes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.ylabel(\"Internal temperature [C]\")\n",
    "plt.xlabel(\"Time [min]\")\n",
    "\n",
    "plt.plot(mins, T, linewidth=3)\n",
    "plt.xlim([np.min(mins), np.max(mins)])\n",
    "\n",
    "plt.savefig(\"%s/temperature_time.png\" % (data_dir), dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Temperature vs Elevation (Wes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"Internal temperature [C]\")\n",
    "plt.ylabel(\"Elevation [km]\")\n",
    "\n",
    "plt.plot(T, alt_gps/1000.0, linewidth=3)\n",
    "plt.savefig(\"%s/elevation_temperature.png\" % (data_dir), dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time interval accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How reliable is our counting loop?\n",
    "mu, std = norm.fit(star_dt)\n",
    "\n",
    "N = len(dt)\n",
    "\n",
    "n, bins, patches = plt.hist(star_dt, bins=32, alpha=1.0, align='left', label=\"%d timer intervals\" % N)\n",
    "\n",
    "# Scale the normed distribution to fit our data\n",
    "dx = bins[1] - bins[0]\n",
    "scale = N*dx\n",
    "\n",
    "# Plot the fit\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "p = scale*norm.pdf(x, mu, std)\n",
    "#plt.plot(x, p, 'r--', linewidth=3, label=\"$\\mu$=%.4f s,  $\\sigma$=%.4f s\" % (mu, std))\n",
    "\n",
    "plt.title(\"Timer Duration\")\n",
    "plt.gca().get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time interval outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just look at the outliers\n",
    "star_dt_o = star_dt[np.where(star_dt != 1.0)]\n",
    "\n",
    "N_o = len(star_dt_o)\n",
    "pct_o = (1.0 * N_o) / (1.0 * N) * 100.0\n",
    "\n",
    "n, bins, patches = plt.hist(star_dt_o, bins=8, alpha=1.0, align='left', label=\"%d timer intervals, %0.1f percent\" % (N_o, pct_o))\n",
    "\n",
    "plt.title(\"Timer Outliers\")\n",
    "plt.gca().get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of counts/sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = df.iloc[:,0].values[1:]\n",
    "seconds = df.index.values[1:]\n",
    "seconds_dt = np.gradient(seconds)\n",
    "\n",
    "cps = counts / seconds_dt\n",
    "cps = np.array(cps[np.where(cps >= 0)], dtype='float64')\n",
    "\n",
    "mincounts = 0\n",
    "\n",
    "cps = cps[cps >= mincounts]\n",
    "N = len(cps)\n",
    "\n",
    "halfn = True\n",
    "\n",
    "if (halfnorm):\n",
    "    mu, std = halfnorm.fit(cps)\n",
    "else:\n",
    "    mu, std = norm.fit(cps)\n",
    "\n",
    "n, bins, patches = plt.hist(cps, bins=8, alpha=1.0, align='mid', label=\"%d samples\" % N)\n",
    "\n",
    "# Scale the normed distribution to fit our data\n",
    "dx = bins[1] - bins[0]\n",
    "\n",
    "plt.xlim([min(cps), max(cps)])\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "if (halfnorm):\n",
    "    scale = N * 2/(np.sqrt(np.pi)) * dx\n",
    "    p = scale * halfnorm.pdf(x, mu, std)\n",
    "else:\n",
    "    scale = N*dx\n",
    "    p = scale*norm.pdf(x, mu, std)\n",
    "    \n",
    "plt.plot(x, p, 'r--', linewidth=3, label=\"$\\mu$=%.3f s$^{-1}$,  $\\sigma$=%.3f s$^{-1}$\" % (mu, std))\n",
    "\n",
    "plt.xlabel(\"Counts per Second\")\n",
    "plt.title(\"Launch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dead time correction of histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seconds = df.index.values[1:]\n",
    "counts = df.iloc[:,0].values[1:]\n",
    "deadt = df.iloc[:,7].values[1:]\n",
    "deadtc = df.iloc[:,8].values[1:]\n",
    "T = df.iloc[:,2].values[1:]\n",
    "P1 = df.iloc[:,4].values[1:]\n",
    "P2 = df.iloc[:,5].values[1:]\n",
    "alt = df.iloc[:,6].values[1:]\n",
    "\n",
    "seconds, seconds_dt, counts, deadt, deadtc, T, P1, P2, alt = enforce_min_counts(seconds, counts, deadt, deadtc, T, P1, P2, alt, 0)\n",
    "\n",
    "avg_deadt = avg_deadtime(deadt, deadtc)\n",
    "\n",
    "counts_c = deadt_correction(seconds_dt, deadtc, avg_deadt)\n",
    "\n",
    "bins = np.linspace(np.min(counts_c), np.max(counts_c), 32)\n",
    "binwidth = bins[1]-bins[0]\n",
    "\n",
    "plt.hist([counts, counts_c], bins, alpha=1.0, align='left', \\\n",
    "         label=[\"%d samples representing %.1f counts\" % (len(counts), sum(counts)), \\\n",
    "                \"%d corrected samples representing %.1f counts\" % (len(counts_c), sum(counts_c))])\n",
    "\n",
    "plt.xlim([np.min(counts_c)-binwidth, np.max(counts_c)+binwidth])\n",
    "\n",
    "plt.xlabel(\"Counts per Second\")\n",
    "plt.title(\"Dead Time Correction, Launch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average dead time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (len(avg_deadt) > 1):\n",
    "    n, bins, patches = plt.hist(avg_deadt, bins=16, alpha=1.0, align='mid', \\\n",
    "         label=\"%d dead time samples\" % len(avg_deadt))\n",
    "    \n",
    "    mu, std = norm.fit(avg_deadt)\n",
    "    \n",
    "    plt.xlim([min(avg_deadt), max(avg_deadt)])\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "    dx = bins[1] - bins[0]\n",
    "    scale = N*dx\n",
    "    p = scale*norm.pdf(x, mu, std)\n",
    "    \n",
    "    plt.plot(x, p, 'r--', linewidth=3, label=\"$\\mu$=%.6f s$^{-1}$,  $\\sigma$=%.6f s$^{-1}$\" % (mu, std))\n",
    "\n",
    "    plt.xlabel(\"Dead Time Per Event (s)\")\n",
    "    plt.title(\"Average Dead Time Per Event\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pressure vs Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(T, P2, 'b-')\n",
    "ax1.set_xlabel('Measured Temperature (C)')\n",
    "\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Measured Pressure (mbar)', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "plt.gca().get_yaxis().get_major_formatter().set_useOffset(False)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(T, alt, 'r-')\n",
    "ax2.set_ylabel('Calculated Altitude (m)', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.gca().get_yaxis().get_major_formatter().set_useOffset(False)\n",
    "\n",
    "plt.xlim(min(T), max(T))\n",
    "\n",
    "#fig.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(seconds-seconds[0], P2, T, c=seconds, cmap='viridis', linewidth=0.5)\n",
    "\n",
    "ax.xaxis.labelpad=30\n",
    "ax.yaxis.labelpad=30\n",
    "ax.zaxis.labelpad=30\n",
    "\n",
    "ax.set_zlabel(\"Temperature [C]\")\n",
    "ax.set_ylabel('Pressure [mb]')\n",
    "ax.set_xlabel('Elapsed Time [s]')\n",
    "\n",
    "plt.gca().get_yaxis().get_major_formatter().set_useOffset(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D plot of time, altitude, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seconds = df.index.values[1:]\n",
    "counts = df.iloc[:,0].values[1:]\n",
    "deadt = df.iloc[:,7].values[1:]\n",
    "deadtc = df.iloc[:,8].values[1:]\n",
    "T = df.iloc[:,2].values[1:]\n",
    "P1 = df.iloc[:,4].values[1:]\n",
    "P2 = df.iloc[:,5].values[1:]\n",
    "alt = df.iloc[:,6].values[1:]\n",
    "\n",
    "P1_min = np.min(P1)\n",
    "P2_min = np.min(P2)\n",
    "T_min = np.min(T)\n",
    "alt_max = np.max(alt)\n",
    "\n",
    "print P1_min, P2_min, T_min, alt_max\n",
    "\n",
    "seconds, seconds_dt, counts, deadt, deadtc, T, P1, P2, alt = enforce_min_counts(seconds, counts, deadt, deadtc, T, P1, P2, alt, 0)\n",
    "\n",
    "avg_deadt = avg_deadtime(deadt, deadtc)\n",
    "\n",
    "counts_c = deadt_correction(seconds_dt, deadtc, avg_deadt)\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(seconds-seconds[0], alt, counts_c, c=seconds, cmap='viridis', linewidth=0.5)\n",
    "\n",
    "ax.set_zlabel(\"Corrected Counts\")\n",
    "ax.set_ylabel('Calculated Altitude (m)')\n",
    "ax.set_xlabel('Elapsed Time (seconds)')\n",
    "plt.gca().get_yaxis().get_major_formatter().set_useOffset(False)\n",
    "\n",
    "plt.title(\"Altitude and Counts vs Elapsed Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrected counts per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seconds = df.index.values[1:]\n",
    "counts = df.iloc[:,0].values[1:]\n",
    "deadt = df.iloc[:,7].values[1:]\n",
    "deadtc = df.iloc[:,8].values[1:]\n",
    "T = df.iloc[:,2].values[1:]\n",
    "P1 = df.iloc[:,4].values[1:]\n",
    "P2 = df.iloc[:,5].values[1:]\n",
    "alt = df.iloc[:,6].values[1:]\n",
    "\n",
    "seconds, seconds_dt, counts, deadt, deadtc, T, P1, P2, alt = enforce_min_counts(seconds, counts, deadt, deadtc, T, P1, P2, alt, 0)\n",
    "\n",
    "avg_deadt = avg_deadtime(deadt, deadtc)\n",
    "\n",
    "counts_c = deadt_correction(seconds_dt, deadtc, avg_deadt)\n",
    "\n",
    "cps = counts_c/seconds_dt\n",
    "cps_avg = np.sum(cps)/len(cps)\n",
    "\n",
    "x = range(int(np.max(seconds)-seconds[0])+1)\n",
    "y = np.ones_like(x)*cps_avg\n",
    "\n",
    "plt.plot(x, cps, 'b-', linewidth='0.5', alpha=0.75, label=\"Counts\")\n",
    "plt.title(\"Corrected Counts per Second\")\n",
    "plt.xlabel(\"Elapsed Time (s)\")\n",
    "plt.ylabel(\"Corrected Counts\")\n",
    "\n",
    "#plt.plot(x, y, 'r-', linewidth=\"3\", label=\"Average CPS: %.3f\" % cps_avg)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.xlim(np.min(x), np.max(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pressure and corrected counts versus elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seconds = df.index.values[1:]\n",
    "counts = df.iloc[:,0].values[1:]\n",
    "deadt = df.iloc[:,7].values[1:]\n",
    "deadtc = df.iloc[:,8].values[1:]\n",
    "T = df.iloc[:,2].values[1:]\n",
    "P1 = df.iloc[:,4].values[1:]\n",
    "P2 = df.iloc[:,5].values[1:]\n",
    "alt = df.iloc[:,6].values[1:]\n",
    "\n",
    "seconds, seconds_dt, counts, deadt, deadtc, T, P1, P2, alt = enforce_min_counts(seconds, counts, deadt, deadtc, T, P1, P2, alt, 0)\n",
    "\n",
    "avg_deadt = avg_deadtime(deadt, deadtc)\n",
    "\n",
    "counts_c = deadt_correction(seconds_dt, deadtc, avg_deadt)\n",
    "\n",
    "# Let's smooth this a bit\n",
    "counts_smoothed = counts_c\n",
    "for i in range(0, 250):\n",
    "    counts_smoothed = savgol_filter(counts_smoothed, 3, 1)\n",
    "\n",
    "elapsed = seconds-seconds[0]\n",
    "                                \n",
    "#plt.plot(seconds, P2)\n",
    "#plt.title(\"Pressure vs Elapsed Time\")\n",
    "#plt.xlabel(\"Elapsed Time (s)\")\n",
    "#plt.ylabel(\"Pressure (mbar)\")\n",
    "#plt.savefig(\"%s\\pressure_time.png\" % (data_dir), bbox_inches='tight')\n",
    "#plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(elapsed, P2, 'b-')\n",
    "ax1.set_xlabel('Elapsed Time (s)')\n",
    "\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Pressure (mbar)', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax1.set_ylim([np.min(P2), np.max(P2)])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(elapsed, counts_smoothed, 'r-')\n",
    "ax2.set_ylabel('Counts', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "ax2.set_ylim([np.min(counts_c), 25])\n",
    "\n",
    "plt.title(\"Pressure and Counts vs Elapsed Time\")\n",
    "plt.xlim(np.min(elapsed), np.max(elapsed))\n",
    "plt.savefig(\"%s\\pressure_counts_time.png\" % (data_dir), bbox_inches='tight')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
